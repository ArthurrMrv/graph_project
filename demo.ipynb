{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Stock Sentiment Graph API - Interactive Demo\n",
    "\n",
    "This notebook showcases the powerful features of the **Stock Sentiment Graph API** - a Neo4j-powered system that analyzes the correlation between social media sentiment and stock price movements.\n",
    "\n",
    "## üìä What This Project Does\n",
    "\n",
    "- **Ingests** stock price data and social media tweets into a Neo4j knowledge graph\n",
    "- **Analyzes** sentiment using Hugging Face FinBERT (financial sentiment model)\n",
    "- **Correlates** social sentiment with stock price movements\n",
    "- **Identifies** trending stocks, influencers, and volatility patterns\n",
    "- **Predicts** price movements based on sentiment trends\n",
    "- **Performs** advanced graph analytics (PageRank, Louvain communities, etc.)\n",
    "\n",
    "## üéØ Key Features We'll Explore\n",
    "\n",
    "1. **Data Pipeline**: Load stocks and tweets into the graph\n",
    "2. **Sentiment Analysis**: Real-time sentiment scoring with FinBERT\n",
    "3. **Quantitative Analysis**: Correlation, trending, predictions, volatility\n",
    "4. **Graph Analytics**: Network influence, communities, cascades\n",
    "5. **Visualizations**: Interactive charts and graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already installed)\n",
    "# !pip install requests pandas matplotlib seaborn plotly ipywidgets networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure API base URL\n",
    "API_BASE = \"http://localhost:8000/api\"\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üåê API Base URL: {API_BASE}\")\n",
    "print(f\"üìÖ Current Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Data Pipeline - Ingesting Stocks & Tweets\n",
    "\n",
    "The unified pipeline loads both stock price data and social media tweets into Neo4j, automatically:\n",
    "- Creating nodes for Stocks, TradingDays, Tweets, HashTags\n",
    "- Extracting hashtags and mentions from tweet text\n",
    "- Calculating daily price changes and volatility\n",
    "- Linking tweets to stocks and trading days\n",
    "- Processing sentiment for tweets (if not already present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_stock_data(stock: str, start_date: str = \"2021-09-30\", end_date: str = \"2022-09-30\"):\n",
    "    \"\"\"\n",
    "    Ingest stock price data and tweets for a given ticker.\n",
    "    This is the main pipeline endpoint that does everything!\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/pipeline/dataset_to_graph\"\n",
    "    payload = {\n",
    "        \"stock\": stock,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"chunk_size\": 2000\n",
    "    }\n",
    "    \n",
    "    print(f\"üîÑ Ingesting data for {stock} from {start_date} to {end_date}...\")\n",
    "    response = requests.post(url, json=payload, timeout=300)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"‚úÖ Success!\")\n",
    "        print(f\"   üìà Prices synced: {result.get('prices_synced', 0)}\")\n",
    "        print(f\"   üê¶ Tweets imported: {result.get('tweets_imported', 0)}\")\n",
    "        if 'sentiment_processing' in result:\n",
    "            sent = result['sentiment_processing']\n",
    "            print(f\"   üí≠ Sentiment processed: {sent.get('processed', 0)} tweets\")\n",
    "        return result\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "# Example: Ingest Tesla (TSLA) data\n",
    "# Uncomment to run (this may take a few minutes)\n",
    "# tsla_result = ingest_stock_data(\"TSLA\", \"2021-09-30\", \"2022-09-30\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Sentiment Analysis - Real-time FinBERT Scoring\n",
    "\n",
    "Analyze sentiment of any text using Hugging Face's FinBERT model, specifically trained on financial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text: str, api_key: str = None):\n",
    "    \"\"\"\n",
    "    Analyze sentiment of text using FinBERT.\n",
    "    Returns sentiment score (0-1, where >0.5 is positive) and confidence.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/sentiment/analyze\"\n",
    "    payload = {\"text\": text}\n",
    "    if api_key:\n",
    "        payload[\"api_key\"] = api_key\n",
    "    \n",
    "    response = requests.post(url, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        return result\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "# Example sentiment analysis\n",
    "sample_tweets = [\n",
    "    \"üöÄ TSLA to the moon! Best investment ever!\",\n",
    "    \"üò∞ TSLA is crashing, sell everything!\",\n",
    "    \"TSLA earnings report shows steady growth\",\n",
    "    \"Not sure about TSLA, mixed signals from analysts\"\n",
    "]\n",
    "\n",
    "print(\"üí≠ Analyzing Sample Tweets:\\n\")\n",
    "for tweet in sample_tweets:\n",
    "    result = analyze_sentiment(tweet)\n",
    "    if result:\n",
    "        sentiment = result['sentiment']\n",
    "        confidence = result['confidence']\n",
    "        label = \"üü¢ Positive\" if sentiment > 0.5 else \"üî¥ Negative\"\n",
    "        print(f\"Tweet: {tweet}\")\n",
    "        print(f\"  {label} | Score: {sentiment:.3f} | Confidence: {confidence:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Quantitative Analysis\n",
    "\n",
    "### 3.1 Sentiment-Price Correlation\n",
    "\n",
    "Discover how social media sentiment correlates with actual stock price movements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_price_correlation(stock: str, start_date: str = None, end_date: str = None):\n",
    "    \"\"\"\n",
    "    Get correlation between sentiment and price movements.\n",
    "    Returns Pearson correlation coefficient and daily data.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/correlation/sentiment-price/{stock}\"\n",
    "    params = {}\n",
    "    if start_date:\n",
    "        params[\"start_date\"] = start_date\n",
    "    if end_date:\n",
    "        params[\"end_date\"] = end_date\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Get correlation data\n",
    "correlation_data = get_sentiment_price_correlation(\"TSLA\", \"2021-09-30\", \"2022-09-30\")\n",
    "\n",
    "if correlation_data:\n",
    "    print(f\"üìä Sentiment-Price Correlation Analysis for {correlation_data['stock']}\")\n",
    "    print(f\"   Correlation Coefficient: {correlation_data.get('correlation_coefficient', 'N/A')}\")\n",
    "    print(f\"   Data Points: {correlation_data.get('data_points', 0)}\")\n",
    "    print(f\"   Interpretation: {correlation_data.get('interpretation', 'N/A')}\")\n",
    "    \n",
    "    # Create visualization\n",
    "    if correlation_data.get('daily_data'):\n",
    "        df = pd.DataFrame(correlation_data['daily_data'])\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "        \n",
    "        # Plot 1: Price and Sentiment over time\n",
    "        ax1_twin = ax1.twinx()\n",
    "        ax1.plot(df['date'], df['close_price'], 'b-', label='Close Price', linewidth=2)\n",
    "        ax1_twin.plot(df['date'], df['avg_sentiment'], 'r-', label='Avg Sentiment', linewidth=2, alpha=0.7)\n",
    "        ax1.set_xlabel('Date', fontsize=12)\n",
    "        ax1.set_ylabel('Close Price ($)', color='b', fontsize=12)\n",
    "        ax1_twin.set_ylabel('Average Sentiment', color='r', fontsize=12)\n",
    "        ax1.set_title('Stock Price vs. Social Media Sentiment Over Time', fontsize=14, fontweight='bold')\n",
    "        ax1.legend(loc='upper left')\n",
    "        ax1_twin.legend(loc='upper right')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Scatter plot showing correlation\n",
    "        ax2.scatter(df['avg_sentiment'], df['close_price'], \n",
    "                   s=df['tweet_count']*2, alpha=0.6, c=df['tweet_count'], cmap='viridis')\n",
    "        ax2.set_xlabel('Average Sentiment', fontsize=12)\n",
    "        ax2.set_ylabel('Close Price ($)', fontsize=12)\n",
    "        ax2.set_title('Sentiment vs. Price Correlation (bubble size = tweet count)', fontsize=14, fontweight='bold')\n",
    "        plt.colorbar(ax2.collections[0], ax=ax2, label='Tweet Count')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Trending Stocks\n",
    "\n",
    "Find which stocks are trending based on tweet volume and sentiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trending_stocks(window: str = \"daily\", limit: int = 10):\n",
    "    \"\"\"\n",
    "    Get trending stocks based on tweet volume and sentiment.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/trending/stocks\"\n",
    "    params = {\"window\": window, \"limit\": limit}\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Get trending stocks\n",
    "trending = get_trending_stocks(\"daily\", 15)\n",
    "\n",
    "if trending:\n",
    "    print(f\"üî• Trending Stocks ({trending['window']} window)\")\n",
    "    print(f\"   Window Start: {trending['start_time']}\\n\")\n",
    "    \n",
    "    df_trending = pd.DataFrame(trending['trending_stocks'])\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Top stocks by trend score\n",
    "    top_10 = df_trending.head(10)\n",
    "    ax1.barh(top_10['ticker'], top_10['trend_score'], color='steelblue')\n",
    "    ax1.set_xlabel('Trend Score', fontsize=12)\n",
    "    ax1.set_title('Top 10 Trending Stocks by Trend Score', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Plot 2: Tweet volume vs sentiment\n",
    "    ax2.scatter(df_trending['avg_sentiment'], df_trending['tweet_volume'], \n",
    "               s=df_trending['trend_score']*10, alpha=0.6, c=df_trending['trend_score'], cmap='coolwarm')\n",
    "    for idx, row in df_trending.head(10).iterrows():\n",
    "        ax2.annotate(row['ticker'], (row['avg_sentiment'], row['tweet_volume']), \n",
    "                    fontsize=8, alpha=0.7)\n",
    "    ax2.set_xlabel('Average Sentiment', fontsize=12)\n",
    "    ax2.set_ylabel('Tweet Volume', fontsize=12)\n",
    "    ax2.set_title('Volume vs. Sentiment (bubble size = trend score)', fontsize=14, fontweight='bold')\n",
    "    plt.colorbar(ax2.collections[0], ax=ax2, label='Trend Score')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display table\n",
    "    print(\"\\nüìã Detailed Trending Stocks Data:\")\n",
    "    print(df_trending.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Top Influencers\n",
    "\n",
    "Identify the most influential users for a specific stock based on their tweet activity and network influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_influencers(stock: str, limit: int = 20):\n",
    "    \"\"\"\n",
    "    Get top influencers for a stock.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/influencers/{stock}\"\n",
    "    params = {\"limit\": limit}\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Get influencers for TSLA\n",
    "influencers = get_top_influencers(\"TSLA\", 15)\n",
    "\n",
    "if influencers:\n",
    "    print(f\"üë• Top Influencers for {influencers['stock']}\\n\")\n",
    "    \n",
    "    df_inf = pd.DataFrame(influencers['top_influencers'])\n",
    "    \n",
    "    if not df_inf.empty:\n",
    "        # Create visualization\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Plot 1: Top influencers by influence score\n",
    "        top_10 = df_inf.head(10)\n",
    "        ax1.barh(range(len(top_10)), top_10['influence_score'], color='coral')\n",
    "        ax1.set_yticks(range(len(top_10)))\n",
    "        ax1.set_yticklabels(top_10['user_id'], fontsize=9)\n",
    "        ax1.set_xlabel('Influence Score', fontsize=12)\n",
    "        ax1.set_title('Top 10 Influencers by Influence Score', fontsize=14, fontweight='bold')\n",
    "        ax1.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Plot 2: Tweet count vs sentiment\n",
    "        ax2.scatter(df_inf['tweet_count'], df_inf['avg_sentiment'], \n",
    "                   s=df_inf['influence_count']*5, alpha=0.6, c=df_inf['influence_score'], cmap='plasma')\n",
    "        ax2.set_xlabel('Tweet Count', fontsize=12)\n",
    "        ax2.set_ylabel('Average Sentiment', fontsize=12)\n",
    "        ax2.set_title('Activity vs. Sentiment (bubble size = influence count)', fontsize=14, fontweight='bold')\n",
    "        plt.colorbar(ax2.collections[0], ax=ax2, label='Influence Score')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Display table\n",
    "        print(\"üìã Detailed Influencers Data:\")\n",
    "        print(df_inf.to_string(index=False))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No influencer data available. Make sure you've ingested data with User information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Sentiment-Based Predictions\n",
    "\n",
    "Predict future price movements based on recent sentiment trends!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_prediction(stock: str, lookback_days: int = 7):\n",
    "    \"\"\"\n",
    "    Get sentiment-based price prediction.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/prediction/sentiment-based/{stock}\"\n",
    "    params = {\"lookback_days\": lookback_days}\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Get prediction for TSLA\n",
    "prediction = get_sentiment_prediction(\"TSLA\", 7)\n",
    "\n",
    "if prediction:\n",
    "    print(f\"üîÆ Sentiment-Based Prediction for {prediction['stock']}\")\n",
    "    print(f\"   Lookback Period: {prediction['lookback_days']} days\")\n",
    "    print(f\"   Prediction: {prediction['prediction'].upper()}\")\n",
    "    print(f\"   Confidence: {prediction['confidence']:.1%}\")\n",
    "    print(f\"   Average Sentiment: {prediction.get('avg_sentiment', 'N/A')}\")\n",
    "    print(f\"   Tweet Volume: {prediction.get('tweet_volume', 0)}\")\n",
    "    print(f\"   Sentiment Volatility: {prediction.get('sentiment_volatility', 'N/A')}\")\n",
    "    print(f\"\\nüí° Interpretation: {prediction.get('interpretation', 'N/A')}\")\n",
    "    \n",
    "    # Visualize prediction\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    pred_type = prediction['prediction']\n",
    "    confidence = prediction['confidence']\n",
    "    \n",
    "    # Color based on prediction\n",
    "    color = 'green' if pred_type == 'bullish' else ('red' if pred_type == 'bearish' else 'gray')\n",
    "    \n",
    "    # Create gauge-like visualization\n",
    "    ax.barh([0], [confidence], color=color, alpha=0.7, height=0.5)\n",
    "    ax.barh([0], [1-confidence], left=[confidence], color='lightgray', alpha=0.3, height=0.5)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(-0.5, 0.5)\n",
    "    ax.set_xlabel('Confidence Level', fontsize=12)\n",
    "    ax.set_title(f'{pred_type.upper()} Prediction - {confidence:.1%} Confidence', \n",
    "                fontsize=14, fontweight='bold', color=color)\n",
    "    ax.text(0.5, 0, f\"{prediction.get('avg_sentiment', 0):.3f} avg sentiment\\n\"\n",
    "           f\"{prediction.get('tweet_volume', 0)} tweets analyzed\",\n",
    "           ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Social-Driven Volatility\n",
    "\n",
    "Identify stocks with high volatility driven by social media sentiment variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_social_volatility(min_tweets: int = 50, limit: int = 20):\n",
    "    \"\"\"\n",
    "    Get stocks with high social-driven volatility.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/volatility/social-driven\"\n",
    "    params = {\"min_tweets\": min_tweets, \"limit\": limit}\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Get volatile stocks\n",
    "volatility = get_social_volatility(50, 20)\n",
    "\n",
    "if volatility:\n",
    "    print(f\"üìà Social-Driven Volatility Analysis\")\n",
    "    print(f\"   Minimum Tweets Threshold: {volatility['min_tweets_threshold']}\\n\")\n",
    "    \n",
    "    df_vol = pd.DataFrame(volatility['volatile_stocks'])\n",
    "    \n",
    "    if not df_vol.empty:\n",
    "        # Create visualization\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Plot 1: Top volatile stocks\n",
    "        top_10 = df_vol.head(10)\n",
    "        ax1.barh(top_10['ticker'], top_10['volatility_score'], color='firebrick')\n",
    "        ax1.set_xlabel('Volatility Score', fontsize=12)\n",
    "        ax1.set_title('Top 10 Most Volatile Stocks (Social-Driven)', fontsize=14, fontweight='bold')\n",
    "        ax1.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Plot 2: Sentiment std vs tweet count\n",
    "        ax2.scatter(df_vol['sentiment_std'], df_vol['tweet_count'], \n",
    "                   s=df_vol['volatility_score']*2, alpha=0.6, c=df_vol['volatility_score'], cmap='Reds')\n",
    "        for idx, row in df_vol.head(10).iterrows():\n",
    "            ax2.annotate(row['ticker'], (row['sentiment_std'], row['tweet_count']), \n",
    "                        fontsize=8, alpha=0.7)\n",
    "        ax2.set_xlabel('Sentiment Standard Deviation', fontsize=12)\n",
    "        ax2.set_ylabel('Tweet Count', fontsize=12)\n",
    "        ax2.set_title('Sentiment Variance vs. Volume (bubble size = volatility score)', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        plt.colorbar(ax2.collections[0], ax=ax2, label='Volatility Score')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Display table with interpretations\n",
    "        print(\"\\nüìã Detailed Volatility Data:\")\n",
    "        display_cols = ['ticker', 'tweet_count', 'avg_sentiment', 'sentiment_std', 'volatility_score', 'interpretation']\n",
    "        available_cols = [col for col in display_cols if col in df_vol.columns]\n",
    "        print(df_vol[available_cols].to_string(index=False))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No volatility data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Graph Analytics\n",
    "\n",
    "### 4.1 Stock Clusters\n",
    "\n",
    "Discover which stocks are clustered together based on hashtag co-occurrence patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_clusters(limit: int = 10):\n",
    "    \"\"\"\n",
    "    Get stock clusters based on hashtag co-occurrence.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/clusters/stocks\"\n",
    "    params = {\"limit\": limit}\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Get stock clusters\n",
    "clusters = get_stock_clusters(15)\n",
    "\n",
    "if clusters:\n",
    "    print(\"üîó Stock Clusters (based on hashtag co-occurrence)\\n\")\n",
    "    \n",
    "    df_clusters = pd.DataFrame(clusters['clusters'])\n",
    "    \n",
    "    if not df_clusters.empty:\n",
    "        try:\n",
    "            import networkx as nx\n",
    "            \n",
    "            # Create network visualization\n",
    "            G = nx.Graph()\n",
    "            for _, row in df_clusters.iterrows():\n",
    "                G.add_edge(row['a'], row['b'], weight=row['score'])\n",
    "            \n",
    "            if len(G.nodes()) > 0:\n",
    "                plt.figure(figsize=(14, 10))\n",
    "                pos = nx.spring_layout(G, k=1, iterations=50)\n",
    "                \n",
    "                # Draw edges with weights\n",
    "                edges = G.edges()\n",
    "                weights = [G[u][v]['weight'] for u, v in edges]\n",
    "                nx.draw_networkx_edges(G, pos, width=[w/10 for w in weights], \n",
    "                                      alpha=0.6, edge_color='gray')\n",
    "                \n",
    "                # Draw nodes\n",
    "                nx.draw_networkx_nodes(G, pos, node_color='steelblue', \n",
    "                                      node_size=1000, alpha=0.9)\n",
    "                \n",
    "                # Draw labels\n",
    "                nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
    "                \n",
    "                plt.title('Stock Clusters Network (edge thickness = co-occurrence score)', \n",
    "                         fontsize=14, fontweight='bold')\n",
    "                plt.axis('off')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è NetworkX not installed. Install with: pip install networkx\")\n",
    "        \n",
    "        # Display table\n",
    "        print(\"\\nüìã Stock Cluster Pairs:\")\n",
    "        print(df_clusters.to_string(index=False))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No cluster data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 GDS Algorithms (Graph Data Science)\n",
    "\n",
    "Advanced graph algorithms using Neo4j's Graph Data Science library.\n",
    "\n",
    "#### 4.2.1 Global Influence (PageRank)\n",
    "Compute global user influence using PageRank algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_influence(limit: int = 20):\n",
    "    \"\"\"\n",
    "    Get global user influence using GDS PageRank.\n",
    "    Requires Neo4j GDS plugin.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/gds/influence/global\"\n",
    "    params = {\"limit\": limit}\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {response.status_code}\")\n",
    "        print(\"Note: This requires Neo4j GDS plugin. Error may indicate GDS is not installed.\")\n",
    "        return None\n",
    "\n",
    "# Get global influence (requires GDS plugin)\n",
    "# Uncomment to run if GDS is installed\n",
    "# global_influence = get_global_influence(20)\n",
    "# if global_influence:\n",
    "#     print(f\"üåç Global Influence Ranking ({global_influence['algorithm']})\\n\")\n",
    "#     df_global = pd.DataFrame(global_influence['top_users'])\n",
    "#     print(df_global.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Stock Communities (Louvain)\n",
    "Detect stock communities using Louvain community detection algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_communities():\n",
    "    \"\"\"\n",
    "    Get stock communities using GDS Louvain algorithm.\n",
    "    Requires Neo4j GDS plugin.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/gds/communities/stocks\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {response.status_code}\")\n",
    "        print(\"Note: This requires Neo4j GDS plugin. Error may indicate GDS is not installed.\")\n",
    "        return None\n",
    "\n",
    "# Get stock communities (requires GDS plugin)\n",
    "# Uncomment to run if GDS is installed\n",
    "# communities = get_stock_communities()\n",
    "# if communities:\n",
    "#     print(f\"üèòÔ∏è Stock Communities ({communities['algorithm']})\\n\")\n",
    "#     df_comm = pd.DataFrame(communities['stocks'])\n",
    "#     \n",
    "#     # Group by community\n",
    "#     for comm_id in sorted(df_comm['communityId'].unique()):\n",
    "#         stocks_in_comm = df_comm[df_comm['communityId'] == comm_id]['ticker'].tolist()\n",
    "#         print(f\"Community {comm_id}: {', '.join(stocks_in_comm)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Comprehensive Analysis Dashboard\n",
    "\n",
    "Let's create a comprehensive dashboard showing multiple metrics for a stock!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stock_dashboard(stock: str, start_date: str = \"2021-09-30\", end_date: str = \"2022-09-30\"):\n",
    "    \"\"\"\n",
    "    Create a comprehensive dashboard for a stock showing:\n",
    "    - Correlation analysis\n",
    "    - Prediction\n",
    "    - Trending status\n",
    "    - Influencers\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä COMPREHENSIVE DASHBOARD: {stock}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # 1. Correlation\n",
    "    print(\"1Ô∏è‚É£ SENTIMENT-PRICE CORRELATION\")\n",
    "    print(\"-\" * 60)\n",
    "    corr = get_sentiment_price_correlation(stock, start_date, end_date)\n",
    "    if corr:\n",
    "        print(f\"   Correlation: {corr.get('correlation_coefficient', 'N/A')}\")\n",
    "        print(f\"   Data Points: {corr.get('data_points', 0)}\")\n",
    "        print(f\"   {corr.get('interpretation', 'N/A')}\\n\")\n",
    "    \n",
    "    # 2. Prediction\n",
    "    print(\"2Ô∏è‚É£ SENTIMENT-BASED PREDICTION\")\n",
    "    print(\"-\" * 60)\n",
    "    pred = get_sentiment_prediction(stock, 7)\n",
    "    if pred:\n",
    "        print(f\"   Prediction: {pred['prediction'].upper()}\")\n",
    "        print(f\"   Confidence: {pred['confidence']:.1%}\")\n",
    "        print(f\"   Avg Sentiment: {pred.get('avg_sentiment', 'N/A')}\")\n",
    "        print(f\"   Tweet Volume: {pred.get('tweet_volume', 0)}\\n\")\n",
    "    \n",
    "    # 3. Trending status\n",
    "    print(\"3Ô∏è‚É£ TRENDING STATUS\")\n",
    "    print(\"-\" * 60)\n",
    "    trending = get_trending_stocks(\"daily\", 50)\n",
    "    if trending:\n",
    "        df_trend = pd.DataFrame(trending['trending_stocks'])\n",
    "        stock_trend = df_trend[df_trend['ticker'] == stock]\n",
    "        if not stock_trend.empty:\n",
    "            row = stock_trend.iloc[0]\n",
    "            rank = df_trend.index[df_trend['ticker'] == stock].tolist()[0] + 1\n",
    "            print(f\"   Rank: #{rank} out of {len(df_trend)} trending stocks\")\n",
    "            print(f\"   Trend Score: {row['trend_score']:.2f}\")\n",
    "            print(f\"   Tweet Volume: {row['tweet_volume']}\")\n",
    "            print(f\"   Avg Sentiment: {row.get('avg_sentiment', 'N/A')}\\n\")\n",
    "        else:\n",
    "            print(f\"   {stock} is not currently trending\\n\")\n",
    "    \n",
    "    # 4. Influencers\n",
    "    print(\"4Ô∏è‚É£ TOP INFLUENCERS\")\n",
    "    print(\"-\" * 60)\n",
    "    inf = get_top_influencers(stock, 5)\n",
    "    if inf and inf.get('top_influencers'):\n",
    "        df_inf = pd.DataFrame(inf['top_influencers'])\n",
    "        print(f\"   Top 5 Influencers:\")\n",
    "        for idx, row in df_inf.head(5).iterrows():\n",
    "            print(f\"   {idx+1}. {row['user_id']} (Score: {row['influence_score']:.2f}, \"\n",
    "                  f\"Tweets: {row['tweet_count']})\")\n",
    "    else:\n",
    "        print(f\"   No influencer data available (requires User nodes)\\n\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Create dashboard for TSLA\n",
    "# Uncomment to run\n",
    "# create_stock_dashboard(\"TSLA\", \"2021-09-30\", \"2022-09-30\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Advanced Visualizations\n",
    "\n",
    "### Multi-Stock Comparison\n",
    "Compare sentiment and price trends across multiple stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_stocks(stocks: List[str], start_date: str = \"2021-09-30\", end_date: str = \"2022-09-30\"):\n",
    "    \"\"\"\n",
    "    Compare multiple stocks side by side.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(len(stocks), 2, figsize=(16, 4*len(stocks)))\n",
    "    \n",
    "    if len(stocks) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, stock in enumerate(stocks):\n",
    "        corr_data = get_sentiment_price_correlation(stock, start_date, end_date)\n",
    "        \n",
    "        if corr_data and corr_data.get('daily_data'):\n",
    "            df = pd.DataFrame(corr_data['daily_data'])\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            \n",
    "            # Plot 1: Price and Sentiment\n",
    "            ax1 = axes[idx, 0]\n",
    "            ax1_twin = ax1.twinx()\n",
    "            ax1.plot(df['date'], df['close_price'], 'b-', label='Price', linewidth=2)\n",
    "            ax1_twin.plot(df['date'], df['avg_sentiment'], 'r-', label='Sentiment', linewidth=2, alpha=0.7)\n",
    "            ax1.set_ylabel('Price ($)', color='b', fontsize=10)\n",
    "            ax1_twin.set_ylabel('Sentiment', color='r', fontsize=10)\n",
    "            ax1.set_title(f'{stock} - Price vs Sentiment', fontsize=12, fontweight='bold')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            ax1.legend(loc='upper left')\n",
    "            ax1_twin.legend(loc='upper right')\n",
    "            \n",
    "            # Plot 2: Correlation scatter\n",
    "            ax2 = axes[idx, 1]\n",
    "            ax2.scatter(df['avg_sentiment'], df['close_price'], \n",
    "                       s=df['tweet_count']*2, alpha=0.6, c=df['tweet_count'], cmap='viridis')\n",
    "            ax2.set_xlabel('Avg Sentiment', fontsize=10)\n",
    "            ax2.set_ylabel('Close Price ($)', fontsize=10)\n",
    "            corr_coef = corr_data.get('correlation_coefficient', 0)\n",
    "            ax2.set_title(f'{stock} - Correlation: {corr_coef:.3f}', fontsize=12, fontweight='bold')\n",
    "            plt.colorbar(ax2.collections[0], ax=ax2, label='Tweets')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compare multiple stocks\n",
    "# Uncomment to run\n",
    "# compare_stocks([\"TSLA\", \"AAPL\", \"MSFT\"], \"2021-09-30\", \"2022-09-30\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Key Takeaways & Use Cases\n",
    "\n",
    "### What Makes This Project Cool? üéØ\n",
    "\n",
    "1. **Unified Graph Model**: Everything (stocks, tweets, users, hashtags) is connected in a Neo4j graph\n",
    "2. **Real-time Sentiment**: FinBERT provides financial-domain sentiment analysis\n",
    "3. **Quantitative Insights**: Correlation, predictions, volatility - all backed by data\n",
    "4. **Graph Analytics**: Network analysis, communities, influence - discover hidden patterns\n",
    "5. **Scalable Pipeline**: Handles large datasets with chunking and batch processing\n",
    "6. **Extensible Schema**: Automatically adapts to additional data (Users, Topics, Events)\n",
    "\n",
    "### Real-World Applications üíº\n",
    "\n",
    "- **Trading Signals**: Use sentiment predictions to inform trading decisions\n",
    "- **Risk Management**: Identify volatile stocks driven by social media\n",
    "- **Market Research**: Understand which stocks are trending and why\n",
    "- **Influencer Marketing**: Identify key influencers for specific stocks\n",
    "- **Portfolio Analysis**: Compare sentiment across your portfolio\n",
    "\n",
    "### Next Steps üöÄ\n",
    "\n",
    "1. **Ingest More Data**: Add more stocks and time periods\n",
    "2. **Custom Analysis**: Build your own queries using Neo4j Cypher\n",
    "3. **Real-time Updates**: Set up streaming to keep data fresh\n",
    "4. **Advanced ML**: Train custom models on the graph data\n",
    "5. **Visualization**: Use Neo4j Bloom or other tools for interactive exploration\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Analyzing! üìàüìäüê¶**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Notes\n",
    "\n",
    "- **API Server**: Make sure the FastAPI server is running (`make run` or `uvicorn app.main:app`)\n",
    "- **Neo4j**: Ensure Neo4j is running (`make up` or Docker)\n",
    "- **Data**: The pipeline uses CSV files in `data/Stock Tweets Sentiment Analysis/`\n",
    "- **GDS Plugin**: Some features require Neo4j Graph Data Science plugin\n",
    "- **Hugging Face**: Sentiment analysis may require HF_TOKEN in `.env` for rate limits\n",
    "\n",
    "### Quick Start Commands\n",
    "\n",
    "```bash\n",
    "# Start Neo4j\n",
    "make up\n",
    "\n",
    "# Run API server\n",
    "make run\n",
    "\n",
    "# Ingest data (via API or curl)\n",
    "curl -X POST \"http://localhost:8000/api/pipeline/dataset_to_graph\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"stock\": \"TSLA\", \"start_date\": \"2021-09-30\", \"end_date\": \"2022-09-30\"}'\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
